{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient(DDPG) Network for tennis Collaborate competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand DDPG\n",
    "DDPG use actor-critic approach based on the DPG algorithm(actor-critic stands for two network here)\n",
    "- actor network: specifies the current policy by deterministically mapping states to a specific action.\n",
    "- critic network: learned by using the Bellman equation as in Q-learning.  \n",
    "\n",
    "DDPG is an off policy alogtithm, which means DDPG collect the states in a replay buffer when the agent interacts with the environment. When training the agent, the buffer randomly yield a batch with respect to (st,at,rt,st+1). We calculate the Q_targets and Q_expected through actor network and critic network respectivlyã€‚ loss is calculate through the mean square difference of Q_targets and Q_expected. Then weights in critic network is update through backprobagation. actor loss is defined as:  \n",
    "`actor_loss = -self.critic_local(states, actions_pred).mean()` which means?? (sorry i don't understand it yet.)  \n",
    "noise is added when select the action, the benefit of noise usage is demonstrate in this paper:Control policy with autocorrelated noise in reinforcement learning for robotics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]           4,352\n",
      "            Linear-2                   [-1, 64]           8,256\n",
      "            Linear-3                    [-1, 4]             260\n",
      "================================================================\n",
      "Total params: 12,868\n",
      "Trainable params: 12,868\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.05\n",
      "Estimated Total Size (MB): 0.05\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuchen/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "from model import Actor,Critic\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "\n",
    "state_size = 33\n",
    "action_size = 4\n",
    "seed = 0\n",
    "model = Actor(state_size, action_size, seed)\n",
    "summary(model, input_size=(33,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]           4,352\n",
      "            Linear-2                   [-1, 64]           8,512\n",
      "            Linear-3                    [-1, 1]              65\n",
      "================================================================\n",
      "Total params: 12,929\n",
      "Trainable params: 12,929\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.05\n",
      "Estimated Total Size (MB): 0.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "state_size = 33\n",
    "action_size = 4\n",
    "seed = 0\n",
    "model = Critic(state_size, action_size, seed)\n",
    "summary(model, [(state_size,),(action_size,)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size  \n",
    "BATCH_SIZE = 512        # minibatch size  \n",
    "GAMMA = 0.99            # discount factor   \n",
    "TAU = 5e-3              # for soft update of target parameters   \n",
    "LR_ACTOR = 1e-4         # learning rate of the actor    \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic    \n",
    "WEIGHT_DECAY = 0        # L2 weight decay   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### here we plot the episode-reward curve durning training\n",
    "![](https://raw.githubusercontent.com/YCyuchen/RL_collab-compet/master/episode-curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Futher work\n",
    "In this work, the (two) agents are trained with the DDPG alogrithm, they share the same networks(actor and critic)during traing and testing.  Another possible solution is to use two actor and two critic networks to train those 2agent seperately.  \n",
    "Besides DDPG, other state of the art algroithm like  PPO, A3C, and D4PG deserved to try and see there performance on this env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
